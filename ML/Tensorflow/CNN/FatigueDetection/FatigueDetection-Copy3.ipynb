{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import l2_regularizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from prettytable import PrettyTable\n",
    "import cv2\n",
    "TRAIN_PIC_PATH = '/Users/apple/Desktop/FatigueDetection/FACE_BAG/MultiPIE/train_image128x128/'\n",
    "TRAIN_LAB_PATH = TRAIN_PIC_PATH + 'Label.txt'\n",
    "TEST_PIC_PATH = '/Users/apple/Desktop/FatigueDetection/FACE_BAG/yalefaces_/'\n",
    "TEST_LAB_PATH = TEST_PIC_PATH + 'Label.txt'\n",
    "PIC_SIZE = 64\n",
    "LEARNING_RATE = 1e-5\n",
    "REGULARIZATION_RATE = 1e-3\n",
    "EPOCHs = 50\n",
    "INPUT_NODE = PIC_SIZE * PIC_SIZE\n",
    "OUTPUT_NODE = 2\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StandardImage(img): \n",
    "    img = cv2.resize(img,(PIC_SIZE,PIC_SIZE))\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229\n",
      "(2019, 4096) (176, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/numpy/core/fromnumeric.py:2652: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.zeros((2019,PIC_SIZE*PIC_SIZE))\n",
    "Y_train = np.zeros((2019,1))\n",
    "with open(TRAIN_PIC_PATH + 'FatigueNum.txt','r') as file:\n",
    "    FatigueNum = np.array(map(int,file.read().split()))\n",
    "with open(TRAIN_PIC_PATH + 'AwakeNum.txt','r') as file:\n",
    "    AwakeNum = np.array(map(int,file.read().split()))\n",
    "cnt_all = 0\n",
    "cnt_awake = 0\n",
    "cnt_tired = 0\n",
    "cnt_tmp = 0\n",
    "print AwakeNum.shape[0]\n",
    "for i in range(2020):\n",
    "        cnt_tmp = 0\n",
    "        if cnt_all == 2019:\n",
    "            break\n",
    "        for j in range(25):\n",
    "                if cnt_all < 2019:\n",
    "                    #print cnt_all\n",
    "                    if (j %5 ==0 or j%5 ==1) and cnt_tired < FatigueNum.shape[0]:\n",
    "                        img = cv2.imread(TRAIN_PIC_PATH + str(FatigueNum[cnt_tired])+'.jpg',0)\n",
    "                        img = StandardImage(img)\n",
    "                        Y_train[cnt_all] = 0\n",
    "                        X_train[cnt_all] = img.flatten()\n",
    "                        cnt_tired+=1\n",
    "                        cnt_all+=1 \n",
    "                        #print 'cnt_all %d cnt_tired %d' % (cnt_all,cnt_tired)\n",
    "                    elif cnt_awake < AwakeNum.shape[0] and cnt_tmp < 15:\n",
    "                        cnt_tmp += 1\n",
    "                       # print cnt_tmp\n",
    "                        img = cv2.imread(TRAIN_PIC_PATH + str(AwakeNum[cnt_awake])+'.jpg',0)\n",
    "                        img = StandardImage(img)\n",
    "                        Y_train[cnt_all] = 1\n",
    "                        X_train[cnt_all] = img.flatten()\n",
    "                        cnt_awake+= 1\n",
    "                        cnt_all += 1\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "Y_train = OneHotEncoder().fit_transform(Y_train).todense()\n",
    "X_test = np.zeros((176,PIC_SIZE*PIC_SIZE))\n",
    "for i in range(176):\n",
    "    image = cv2.imread(TEST_PIC_PATH+str(i+1)+'.jpg',0)\n",
    "    image = StandardImage(image)\n",
    "    X_test[i] = image.flatten()\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = np.loadtxt(TEST_LAB_PATH).astype('int').reshape((-1,1))\n",
    "Y_test = OneHotEncoder().fit_transform(Y_test).todense()\n",
    "print X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Inputs'):\n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE],name = 'x_in')\n",
    "    y = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name = 'y_in')\n",
    "    keep_prob = tf.placeholder(tf.float32,name = 'keep_prob')\n",
    "    x_image = tf.reshape(x,[-1,PIC_SIZE,PIC_SIZE,1],name = 'x_in_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial,name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial,name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,W,name):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding = 'SAME',name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x,name):\n",
    "    return tf.nn.max_pool(x, ksize =[1,2,2,1], strides=[1,2,2,1] ,padding = 'SAME',name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv1'):\n",
    "    W_conv1 = weight_variable([5,5,1,32],'W_conv1')\n",
    "    b_conv1 = bias_variable([32],'b_conv1')\n",
    "    conv1 = conv2d(x_image,W_conv1,'conv1')\n",
    "    h_conv1 = tf.nn.relu(conv1 + b_conv1,name = 'Relu_1')\n",
    "    h_pool1 = max_pool_2x2(h_conv1,'h_pool1')\n",
    "    tf.summary.histogram('W_conv1',W_conv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv2'):\n",
    "    W_conv2 = weight_variable([3,3,32,64],'W_conv2')\n",
    "    b_conv2 = bias_variable([64],'b_conv2')\n",
    "    conv2 = conv2d(h_pool1,W_conv2,'conv2')\n",
    "    h_conv2 = tf.nn.relu(conv2 + b_conv2,name = 'Relu_2')\n",
    "    h_pool2 = max_pool_2x2(h_conv2,'h_pool2')\n",
    "    tf.summary.histogram('W_conv2',W_conv2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv3'):\n",
    "    W_conv3 = weight_variable([5,5,64,128],'W_conv3')\n",
    "    b_conv3 = bias_variable([128],'b_conv3')\n",
    "    conv3 = conv2d(h_pool2,W_conv3,'conv3')\n",
    "    h_conv3 = tf.nn.relu(conv3 + b_conv3,name = 'Relu_3')\n",
    "    h_pool3 = max_pool_2x2(h_conv3,'h_pool3')\n",
    "    tf.summary.histogram('W_conv3',W_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([8*8*128,512],'W_fc1')\n",
    "    b_fc1 = bias_variable([512],'b_fc1')\n",
    "    h_pool3_flat = tf.reshape(h_pool3,[-1,8*8*128],name = 'h_pool3_flat')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat,W_fc1) + b_fc1)\n",
    "    tf.summary.histogram('W_fc1',W_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('DropOut'):\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name = 'h_fc1_drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('output'):\n",
    "    W_fc2 = weight_variable([512,OUTPUT_NODE],'W_fc2')\n",
    "    b_fc2 = bias_variable([OUTPUT_NODE],'b_fc2')\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2) + b_fc2,name = 'Relu_4')\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2,name = 'y_conv2')\n",
    "    tf.summary.histogram('W_fc2',W_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_conv,labels = y),name = 'cross_entropy')\n",
    "    l2 = l2_regularizer(REGULARIZATION_RATE)\n",
    "    loss = cross_entropy + l2(W_conv1) + l2(W_conv2) + l2(W_conv3) + l2(W_fc1) + l2(W_fc2)\n",
    "    tf.summary.scalar('loss',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train_step'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('prediction'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    tf.summary.scalar('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X,Y,step,batch_size):\n",
    "    for i in range(step // batch_size + 1):\n",
    "        batch_x = X[i*batch_size:min(i*batch_size + batch_size,step)]\n",
    "        batch_y = Y[i*batch_size:min(i*batch_size + batch_size,step)]\n",
    "        yield batch_x,batch_y,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0-0 train_acc : 0.6\n",
      "step 0-10 train_acc : 0.6\n",
      "step 0-20 train_acc : 0.6\n",
      "step 0-30 train_acc : 0.6\n",
      "step 0-40 train_acc : 0.6\n",
      "step 0-50 train_acc : 0.6\n",
      "step 0-60 train_acc : 0.6\n",
      "step 0-70 train_acc : 0.6\n",
      "step 0-80 train_acc : 1\n",
      "step 1-0 train_acc : 0.6\n",
      "step 1-10 train_acc : 0.6\n",
      "step 1-20 train_acc : 0.6\n",
      "step 1-30 train_acc : 0.6\n",
      "step 1-40 train_acc : 0.6\n",
      "step 1-50 train_acc : 0.6\n",
      "step 1-60 train_acc : 0.6\n",
      "step 1-70 train_acc : 0.6\n",
      "step 1-80 train_acc : 1\n",
      "step 2-0 train_acc : 0.6\n",
      "step 2-10 train_acc : 0.6\n",
      "step 2-20 train_acc : 0.6\n",
      "step 2-30 train_acc : 0.6\n",
      "step 2-40 train_acc : 0.6\n",
      "step 2-50 train_acc : 0.6\n",
      "step 2-60 train_acc : 0.6\n",
      "step 2-70 train_acc : 0.6\n",
      "step 2-80 train_acc : 1\n",
      "step 3-0 train_acc : 0.6\n",
      "step 3-10 train_acc : 0.6\n",
      "step 3-20 train_acc : 0.6\n",
      "step 3-30 train_acc : 0.6\n",
      "step 3-40 train_acc : 0.6\n",
      "step 3-50 train_acc : 0.6\n",
      "step 3-60 train_acc : 0.6\n",
      "step 3-70 train_acc : 0.6\n",
      "step 3-80 train_acc : 1\n",
      "step 4-0 train_acc : 0.6\n",
      "step 4-10 train_acc : 0.6\n",
      "step 4-20 train_acc : 0.6\n",
      "step 4-30 train_acc : 0.6\n",
      "step 4-40 train_acc : 0.6\n",
      "step 4-50 train_acc : 0.6\n",
      "step 4-60 train_acc : 0.6\n",
      "step 4-70 train_acc : 0.6\n",
      "step 4-80 train_acc : 1\n",
      "step 5-0 train_acc : 0.6\n",
      "step 5-10 train_acc : 0.6\n",
      "step 5-20 train_acc : 0.6\n",
      "step 5-30 train_acc : 0.6\n",
      "step 5-40 train_acc : 0.6\n",
      "step 5-50 train_acc : 0.6\n",
      "step 5-60 train_acc : 0.6\n",
      "step 5-70 train_acc : 0.6\n",
      "step 5-80 train_acc : 1\n",
      "step 6-0 train_acc : 0.6\n",
      "step 6-10 train_acc : 0.6\n",
      "step 6-20 train_acc : 0.6\n",
      "step 6-30 train_acc : 0.6\n",
      "step 6-40 train_acc : 0.6\n",
      "step 6-50 train_acc : 0.6\n",
      "step 6-60 train_acc : 0.6\n",
      "step 6-70 train_acc : 0.6\n",
      "step 6-80 train_acc : 1\n",
      "step 7-0 train_acc : 0.6\n",
      "step 7-10 train_acc : 0.6\n",
      "step 7-20 train_acc : 0.6\n",
      "step 7-30 train_acc : 0.6\n",
      "step 7-40 train_acc : 0.6\n",
      "step 7-50 train_acc : 0.6\n",
      "step 7-60 train_acc : 0.6\n",
      "step 7-70 train_acc : 0.6\n",
      "step 7-80 train_acc : 1\n",
      "step 8-0 train_acc : 0.6\n",
      "step 8-10 train_acc : 0.6\n",
      "step 8-20 train_acc : 0.6\n",
      "step 8-30 train_acc : 0.6\n",
      "step 8-40 train_acc : 0.6\n",
      "step 8-50 train_acc : 0.6\n",
      "step 8-60 train_acc : 0.6\n",
      "step 8-70 train_acc : 0.6\n",
      "step 8-80 train_acc : 1\n",
      "step 9-0 train_acc : 0.6\n",
      "step 9-10 train_acc : 0.6\n",
      "step 9-20 train_acc : 0.6\n",
      "step 9-30 train_acc : 0.6\n",
      "step 9-40 train_acc : 0.6\n",
      "step 9-50 train_acc : 0.6\n",
      "step 9-60 train_acc : 0.6\n",
      "step 9-70 train_acc : 0.6\n",
      "step 9-80 train_acc : 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(10):\n",
    "        for bx,by,i in get_batch(X_train,Y_train,Y_train.shape[0],25):\n",
    "            if i % 10 == 0:\n",
    "                train_acc  = sess.run(accuracy,feed_dict={x:bx,y:by,keep_prob:1.0})\n",
    "                print 'step %d-%d train_acc : %g' % (epoch,i,train_acc)\n",
    "            summary,_  = sess.run([merged,train_step],feed_dict={x:bx,y:by,keep_prob:0.75})\n",
    "    writer.add_summary(summary,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-95892c41e246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'step %d-%d train_acc : %g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrintReport(real,predict,length):\n",
    "    TP = 0 \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(length):\n",
    "        TP += 1 if((real[i] == 1) and (real[i] == predict[i])) else 0\n",
    "        TN += 1 if((real[i] == 0) and (real[i] == predict[i])) else 0\n",
    "        FP  += 1 if((predict[i] == 1) and (real[i] != predict[i])) else 0\n",
    "        FN += 1 if((predict[i] == 0) and (real[i] !=predict[i])) else 0\n",
    "    precision_P = (TP*1.0/(TP+FP)) if TP+FP != 0 else 0\n",
    "    precision_N = TN*1.0/(TN+FN) if TN+FN !=0 else 0\n",
    "    recall_P = TP*1.0/(TP+FN) if TP+FN !=0 else 0\n",
    "    recall_N = TN*1.0/(TN+FP) if TN+FP !=0 else 0\n",
    "    f1_P = (2.0/(1.0/precision_P+1.0/recall_P)) if precision_P!=0 and recall_P!=0 else 0\n",
    "    f1_N = 2.0/(1.0/precision_N+1.0/recall_N) if precision_N !=0 and recall_N !=0 else 0\n",
    "    header = 'Label precision recall f1-score support'.split()\n",
    "    row1 = ['0',precision_N,recall_N,f1_N,TN+FP]\n",
    "    row2= ['1',precision_P,recall_P,f1_P,TP+FN]\n",
    "    row3=['avg/total',0,0,0,row1[4]+row2[4]]\n",
    "    for i in range(1,4):\n",
    "        row3[i]=(row1[i]+row2[i])/2.0\n",
    "    pt = PrettyTable()\n",
    "    pt._set_field_names(header)\n",
    "    pt.add_row(row1)\n",
    "    pt.add_row(row2)\n",
    "    pt.add_row(row3)\n",
    "    acc = (TP+TN)*1.0/(length)\n",
    "    print 'Test Accuracy '+str(acc)\n",
    "    print str(pt)+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-533e4bba48f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mPrintReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(y_conv,1)\n",
    "Y_now = tf.argmax(Y_test,1)\n",
    "result = np.zeros(X_test.shape[0]).astype(int)\n",
    "for i in range(X_test.shape[0]//25+1):\n",
    "    tmp = sess.run(prediction,feed_dict = {x:X_test[i*25:min(i*25+25,X_test.shape[0])],keep_prob:1.0})\n",
    "    result[i*25:min(i*25+25,X_test.shape[0])] = (np.array(tmp).astype(int))\n",
    "PrintReport(Y_now.eval(),result,Y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result= []\n",
    "for i in range(X_train.shape[0]//25+1):\n",
    "    tmp = sess.run(prediction,feed_dict = {x:X_train[i*25:min(i*25+25,X_train.shape[0])],keep_prob:1.0})\n",
    "    result[i*25:min(i*25+25,X_train.shape[0])] = (np.array(tmp).astype(int))\n",
    "Y_now = tf.argmax(Y_train,1)\n",
    "PrintReport(Y_now.eval(),result,Y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
